{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8c5539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for numerical operations, plotting, TensorFlow, and image processing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input # Specific preprocessing for VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import callbacks, layers, models, optimizers, regularizers # Keras components for model building\n",
    "import os # Operating system functionalities, e.g., path joining\n",
    "import cv2 # OpenCV for image processing tasks\n",
    "from keras.losses import SparseCategoricalCrossentropy # Loss function for multi-class classification\n",
    "from PIL import Image # Python Imaging Library for image manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5e77c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape for the images (height, width, channels)\n",
    "input_shape = (64, 64, 3)\n",
    "# Define batch size for training and evaluation\n",
    "batch_size = 64\n",
    "# Define the number of epochs for training; EarlyStopping may halt training sooner\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3924cdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Training Generator...\n",
      "Found 30580 images belonging to 10 classes.\n",
      "Found 30580 images belonging to 10 classes in training set.\n",
      "\n",
      "Setting up Validation Generator...\n",
      "Found 328 images belonging to 10 classes.\n",
      "Found 328 images belonging to 10 classes in validation set.\n",
      "\n",
      "Setting up Testing Generator...\n",
      "Found 328 images belonging to 10 classes.\n",
      "Found 328 images belonging to 10 classes in validation set.\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the preprocessed RGB images directory\n",
    "preprocessed_path = r\"D:\\FUCK!!\\Pattern\\Project\\notebooks\\preprocessed_RGB_images\"\n",
    "\n",
    "# Initialize ImageDataGenerator with VGG16 preprocessing function\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "# Setup the training data generator\n",
    "print(\"Setting up Training Generator...\")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    os.path.join(preprocessed_path, 'train'), # Path to the training data\n",
    "    target_size=input_shape[:2], # Resize images to the defined input_shape\n",
    "    batch_size=batch_size, # Set the batch size\n",
    "    class_mode='sparse', # Use 'sparse' for sparse_categorical_crossentropy loss\n",
    "    color_mode='rgb', # Ensure images are loaded in RGB format\n",
    "    shuffle=True # Shuffle the training data\n",
    ")\n",
    "print(f\"Found {train_generator.samples} images belonging to {train_generator.num_classes} classes in training set.\")\n",
    "\n",
    "# Setup the validation data generator\n",
    "print(\"\\nSetting up Validation Generator...\")\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    os.path.join(preprocessed_path, 'val'), # Path to the validation data\n",
    "    target_size=input_shape[:2], # Resize images\n",
    "    batch_size=batch_size, # Set batch size\n",
    "    class_mode='sparse', # Use 'sparse' for loss function\n",
    "    color_mode='rgb', # Load images in RGB\n",
    "    shuffle=False # No need to shuffle validation data\n",
    ")\n",
    "print(f\"Found {val_generator.samples} images belonging to {val_generator.num_classes} classes in validation set.\")\n",
    "\n",
    "# Setup the testing data generator\n",
    "print(\"\\nSetting up Testing Generator...\")\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    os.path.join(preprocessed_path, 'test'), # Path to the test data\n",
    "    target_size=input_shape[:2], # Resize images\n",
    "    batch_size=batch_size, # Set batch size\n",
    "    class_mode='sparse', # Use 'sparse' for loss function\n",
    "    color_mode='rgb', # Load images in RGB\n",
    "    shuffle=False # No need to shuffle test data\n",
    ")\n",
    "print(f\"Found {test_generator.samples} images belonging to {test_generator.num_classes} classes in validation set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb037600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model architecture using Keras Sequential API\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=input_shape), # Define the input layer with the specified shape\n",
    "\n",
    "    # Convolutional Block 1\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'), # 32 filters, 3x3 kernel, ReLU activation\n",
    "    layers.BatchNormalization(), # Normalize activations\n",
    "    layers.MaxPooling2D(2), # Max pooling with 2x2 pool size\n",
    "\n",
    "    # Convolutional Block 2\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'), # 64 filters\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2),\n",
    "\n",
    "    # Convolutional Block 3\n",
    "    layers.Conv2D(128, 3, padding='same', activation='relu'), # 128 filters\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2),\n",
    "\n",
    "    # Convolutional Block 4\n",
    "    layers.Conv2D(256, 3, padding='same', activation='relu'), # 256 filters\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2),\n",
    "\n",
    "    # Convolutional Block 5\n",
    "    layers.Conv2D(512, 3, padding='same', activation='relu'), # 512 filters\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2),\n",
    "\n",
    "    # Classification Head\n",
    "    layers.GlobalAveragePooling2D(), # Global average pooling to flatten feature maps\n",
    "    layers.Dense(512, activation='relu'), # Fully connected layer with 512 units\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5), # Dropout for regularization (50% dropout rate)\n",
    "\n",
    "    layers.Dense(256, activation='relu'), # Fully connected layer with 256 units\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5), # Dropout for regularization\n",
    "    layers.Dense(10, activation='softmax', name='predictions') # Output layer with 10 units (for 10 classes) and softmax activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "# Use Adam optimizer with a specific, smaller learning rate for fine-tuning or stable training\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy', # Loss function for integer labels\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "print(\"\\nModel Summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51a57858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Callbacks for training\n",
    "# Early stopping to prevent overfitting and stop training when validation loss stops improving\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', # Metric to monitor\n",
    "    patience=5, # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1, # Log when training is stopped\n",
    "    restore_best_weights=True # Restore model weights from the epoch with the best validation loss\n",
    ")\n",
    "\n",
    "# Model checkpoint to save the best model found during training based on validation loss\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath='best_CNN_model.keras', # File path to save the model\n",
    "    monitor='val_loss', # Metric to monitor\n",
    "    save_best_only=True, # Only save a model if `val_loss` has improved\n",
    "    verbose=1 # Log when a model is saved\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8019150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the training and validation generators\n",
    "print(f\"\\nStarting training for up to {num_epochs} epochs (head only)...\")\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size, # Number of steps per epoch\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_generator.samples // batch_size, # Number of validation steps\n",
    "    epochs=num_epochs, # Maximum number of epochs\n",
    "    callbacks=[early_stopping, model_checkpoint] # List of callbacks to use during training\n",
    ")\n",
    "print(\"\\nTraining finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "413ba2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best saved model for evaluation or further use\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(r'D:\\FUCK!!\\Pattern\\Project\\Models\\best_CNN_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff2ed29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 328 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for model evaluation\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Prepare a test data generator with only preprocessing (no augmentation)\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "test_gen = test_datagen.flow_from_directory(\n",
    "    directory=r'D:\\FUCK!!\\Pattern\\Project\\notebooks\\preprocessed_RGB_images\\test', # Path to the test dataset\n",
    "    target_size=input_shape[:2], # Resize images to the model's input shape\n",
    "    batch_size=batch_size, # Batch size for evaluation\n",
    "    class_mode='sparse', # For integer labels\n",
    "    shuffle=False # Important: Do not shuffle for evaluation to align predictions with true labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "041b7778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any old Keras session/graph to avoid conflicts\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Reload the saved .keras model\n",
    "model = load_model(r'D:\\FUCK!!\\Pattern\\Project\\Models\\best_CNN_model.keras')\n",
    "\n",
    "# Re-compile the model with the correct loss, metrics, and enable eager execution for evaluation\n",
    "# Eager execution can sometimes help with debugging or specific evaluation scenarios\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4), # Adam optimizer with a small learning rate\n",
    "    loss='sparse_categorical_crossentropy', # Loss function for integer labels\n",
    "    metrics=['accuracy'], # Metric to evaluate\n",
    "    run_eagerly=True # Disables tf.function wrapping, runs operations eagerly\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test generator\n",
    "loss, acc = model.evaluate(test_gen, verbose=1)\n",
    "print(f\"\\nTest loss: {loss:.4f} â€” Test accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87514eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain per-class metrics\n",
    "# Calculate the number of steps needed to cover the entire test set\n",
    "steps = int(np.ceil(test_gen.samples / batch_size))\n",
    "\n",
    "# Predict class probabilities for the test set\n",
    "pred_probs = model.predict(\n",
    "    test_gen,\n",
    "    steps = steps, # Ensure all test samples are predicted\n",
    "    verbose=1 # Show prediction progress\n",
    ")\n",
    "\n",
    "# Convert predicted probabilities to class indices by taking the argmax\n",
    "pred_idxs = np.argmax(pred_probs, axis=1)\n",
    "\n",
    "# Get true class indices from the test generator\n",
    "true_idxs = test_gen.classes\n",
    "\n",
    "# Get the list of class labels (names)\n",
    "labels = list(test_gen.class_indices.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae91a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and print the classification report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(true_idxs, pred_idxs, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9eadcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and print the confusion matrix\n",
    "print(\"\\nConfusion Matrix:\\n\")\n",
    "print(confusion_matrix(true_idxs, pred_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a5f40d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(); \n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4e9969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(); \n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
